{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import json\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import pytz\n",
    "%matplotlib inline\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# training and Testing directories\n",
    "training_dir = os.path.join(\"Datasets\", \"Training\")\n",
    "testing_dir = os.path.join(\"Datasets\", \"Testing\")\n",
    "if not os.path.isdir(training_dir):\n",
    "    raise Exception(\"ERROR: training dataset not found\")\n",
    "if not os.path.isdir(testing_dir):\n",
    "    raise Exception(\"ERROR: testing dataset not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets/Training/tweets_#gohawks.txt\n",
      "Datasets/Training/tweets_#gopatriots.txt\n",
      "Datasets/Training/tweets_#nfl.txt\n",
      "Datasets/Training/tweets_#patriots.txt\n",
      "Datasets/Training/tweets_#sb49.txt\n",
      "Datasets/Training/tweets_#superbowl.txt\n"
     ]
    }
   ],
   "source": [
    "# iterate over all hashtag files \n",
    "for root, dirs, files in os.walk(training_dir, topdown=False):\n",
    "    for file in files:\n",
    "        print(os.path.join(root, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialize dictionaries and Unix times for Feb 1, 8 am and Feb 1, 8 pm. \n",
    "# Dictionary keys: hashtag.\n",
    "# Dictionary values: [time of tweet (Unix), number of retweets for tweet, number of followers for tweeter]\n",
    "\n",
    "hashtag_dict_before = {}\n",
    "hashtag_dict_during = {}\n",
    "hashtag_dict_after = {}\n",
    "start_unix_time = 1422806400\n",
    "end_unix_time = 1422849600\n",
    "pst_tz = pytz.timezone('America/Los_Angeles')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing gohawks...\n",
      "Parsing gopatriots...\n",
      "Parsing nfl...\n",
      "Parsing patriots...\n",
      "Parsing sb49...\n",
      "Parsing superbowl...\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "for root, dirs, files in os.walk(training_dir, topdown=False):\n",
    "    for file in files:\n",
    "        filename = os.path.splitext(file)[0].replace('tweets_#', '')\n",
    "        print('Parsing {}...'.format(filename))\n",
    "        \n",
    "        hashtag_dict_before[filename] = []\n",
    "        hashtag_dict_during[filename] = []\n",
    "        hashtag_dict_after[filename] = []\n",
    "        \n",
    "        # open the file and read all lines:\n",
    "        with open(os.path.join(root, file), \"r\", encoding=\"utf-8\") as hashtag:\n",
    "            # read line-by-line\n",
    "            for line in hashtag:\n",
    "                json_obj = json.loads(line)\n",
    "                \n",
    "                # get desired statistics\n",
    "                citation_date = json_obj['citation_date'] # Unix time\n",
    "                num_retweets = json_obj['metrics']['citations']['total'] # Number of retweets for this tweet\n",
    "                num_followers = json_obj['author']['followers'] # Number of followers for tweeter\n",
    "                \n",
    "                if citation_date < start_unix_time:\n",
    "                    hashtag_dict_before[filename].append([citation_date, num_retweets, num_followers])\n",
    "                elif citation_date > end_unix_time:\n",
    "                    hashtag_dict_after[filename].append([citation_date, num_retweets, num_followers])\n",
    "                else:\n",
    "                    hashtag_dict_during[filename].append([citation_date, num_retweets, num_followers])\n",
    "    print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sort each by time.\n",
    "\n",
    "hashtags = ['gohawks', 'gopatriots', 'nfl', 'patriots', 'sb49', 'superbowl']\n",
    "\n",
    "for key in hashtags:\n",
    "    hashtag_dict_before[key] = np.array(hashtag_dict_before[key])\n",
    "#     indices = np.argsort(hashtag_dict_before[key][:,0])\n",
    "#     hashtag_dict_before[key] = hashtag_dict_before[key][indices]\n",
    "    \n",
    "    hashtag_dict_during[key] = np.array(hashtag_dict_during[key])\n",
    "#     indices = np.argsort(hashtag_dict_during[key][:,0])\n",
    "#     hashtag_dict_during[key] = hashtag_dict_during[key][indices]\n",
    "\n",
    "    hashtag_dict_after[key] = np.array(hashtag_dict_after[key])\n",
    "#     indices = np.argsort(hashtag_dict_after[key][:,0])\n",
    "#     hashtag_dict_after[key] = hashtag_dict_after[key][indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find how many time windows there are\n",
    "\n",
    "ftt = int(np.min([np.min(hashtag_dict_before[key][:,0]) for key in hashtags])) # first tweet time\n",
    "ltt = int(np.max([np.max(hashtag_dict_after[key][:,0]) for key in hashtags])) # last tweet time\n",
    "\n",
    "num_windows_before = int(np.max([((start_unix_time - ftt) // 3600) + 1 for key in hashtags]))\n",
    "num_windows_during = int(np.max([((end_unix_time - start_unix_time) // 3600 * 12) for key in hashtags]))\n",
    "num_windows_after = int(np.max([((ltt - end_unix_time) // 3600) + 1 for key in hashtags]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gohawks\n",
      "gopatriots\n",
      "nfl\n",
      "patriots\n",
      "sb49\n",
      "superbowl\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "data_hashtag_before = {}\n",
    "data_hashtag_during = {}\n",
    "data_hashtag_after = {}\n",
    "\n",
    "for key in hashtags:\n",
    "    print(key)\n",
    "    \n",
    "    # Rename the dictionary value for readability\n",
    "    temp_before = hashtag_dict_before[key]\n",
    "    temp_during = hashtag_dict_during[key]\n",
    "    temp_after = hashtag_dict_after[key]\n",
    "        \n",
    "    # Iterate through all elements before start time\n",
    "    data_hashtag_before[key] = np.zeros((num_windows_before, 5))\n",
    "    num_followers_before = {}\n",
    "    for i in range(np.shape(temp_before)[0]):\n",
    "        item_before = int(num_windows_before - 1 - ((start_unix_time - temp_before[i,0] - 1) // 3600))\n",
    "        data_hashtag_before[key][item_before] += np.array([1, int(temp_before[i, 1]), int(temp_before[i, 2]), 0, 0])\n",
    "#         data_hashtag_before[key][item_before][3] = np.max(temp_before[:,2])\n",
    "        dt_obj_pst = datetime.fromtimestamp(temp_before[i,0], pst_tz)\n",
    "        data_hashtag_before[key][item_before][4] = int(datetime.strftime(dt_obj_pst, '%H'))\n",
    "        \n",
    "        if item_before not in num_followers_before.keys():\n",
    "            num_followers_before[item_before] = []\n",
    "        num_followers_before[item_before].append(temp_before[i,2])\n",
    "    for i in num_followers_before.keys():\n",
    "        data_hashtag_before[key][i][3] = np.max(num_followers_before[i])\n",
    "        \n",
    "    # Iterate through all elements during time\n",
    "    data_hashtag_during[key] = np.zeros((num_windows_during, 5))\n",
    "    num_followers_during = {}\n",
    "    for i in range(np.shape(temp_during)[0]):\n",
    "        item_during = int(((temp_during[i,0] - start_unix_time) * 12) // 3600)\n",
    "        data_hashtag_during[key][item_during] += np.array([1, int(temp_during[i, 1]), int(temp_during[i, 2]), 0, 0])\n",
    "#         data_hashtag_during[key][item_during][3] = np.max(temp_during[:,2])\n",
    "        dt_obj_pst = datetime.fromtimestamp(temp_during[i,0], pst_tz)\n",
    "        data_hashtag_during[key][item_during][4] = int(datetime.strftime(dt_obj_pst, '%H'))\n",
    "        \n",
    "        if item_during not in num_followers_during.keys():\n",
    "            num_followers_during[item_during] = []\n",
    "        num_followers_during[item_during].append(temp_during[i,2])\n",
    "    for i in num_followers_during.keys():\n",
    "        data_hashtag_during[key][i][3] = np.max(num_followers_during[i])\n",
    "        \n",
    "    # Iterate through all elements after end time\n",
    "    data_hashtag_after[key] = np.zeros((num_windows_after, 5))\n",
    "    num_followers_after = {}\n",
    "    for i in range(np.shape(temp_after)[0]):\n",
    "        item_after = int((temp_after[i,0] - end_unix_time) // 3600)\n",
    "        data_hashtag_after[key][item_after] += np.array([1, int(temp_after[i, 1]), int(temp_after[i, 2]), 0, 0])\n",
    "#         data_hashtag_after[key][item_after][3] = np.max(temp_after[:,2])\n",
    "        dt_obj_pst = datetime.fromtimestamp(temp_after[i,0], pst_tz)\n",
    "        data_hashtag_after[key][item_after][4] = int(datetime.strftime(dt_obj_pst, '%H'))\n",
    "        \n",
    "        if item_after not in num_followers_after.keys():\n",
    "            num_followers_after[item_after] = []\n",
    "        num_followers_after[item_after].append(temp_after[i,2])\n",
    "    for i in num_followers_after.keys():\n",
    "        data_hashtag_after[key][i][3] = np.max(num_followers_after[i])\n",
    "        \n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Aggregate data\n",
    "\n",
    "# Initialize aggregated data variables\n",
    "data_aggregate_before = np.zeros([num_windows_before, 5])\n",
    "data_aggregate_during = np.zeros([num_windows_during, 5])\n",
    "data_aggregate_after = np.zeros([num_windows_after, 5])\n",
    "\n",
    "# Sum the # of tweets, total # of retweets, and # of followers\n",
    "for key in hashtags:\n",
    "    data_aggregate_before[:,0:3] += data_hashtag_before[key][:,0:3]\n",
    "    data_aggregate_during[:,0:3] += data_hashtag_during[key][:,0:3]\n",
    "    data_aggregate_after[:,0:3] += data_hashtag_after[key][:,0:3]\n",
    "# Find the max # of followers for each\n",
    "data_aggregate_before[:,3] = np.amax([data_hashtag_before[key][:,3] for key in hashtags], axis=0)\n",
    "data_aggregate_during[:,3] = np.amax([data_hashtag_during[key][:,3] for key in hashtags], axis=0)\n",
    "data_aggregate_after[:,3] = np.amax([data_hashtag_after[key][:,3] for key in hashtags], axis=0)\n",
    "\n",
    "# Copy over the same time frames\n",
    "data_aggregate_before[:,4] = data_hashtag_before['superbowl'][:,4]\n",
    "data_aggregate_during[:,4] = data_hashtag_during['superbowl'][:,4]\n",
    "data_aggregate_after[:,4] = data_hashtag_after['superbowl'][:,4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "layer_sizes = [(50, 50), (100, 100), (100, 100, 100), (100, 100, 100, 100), 10*(50,), 10*(100,)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def analyze_nn(nn, X, y, layers):\n",
    "    mses_per_layer = []\n",
    "    kf = KFold(10)\n",
    "    for trainset, testset in kf.split(X):\n",
    "        X_train, y_train = X[trainset], y[trainset]\n",
    "        X_test, y_test = X[testset], y[testset]\n",
    "        nn.fit(X_train, y_train)\n",
    "        predicted = nn.predict(X_test)\n",
    "        mses_per_layer.append(mean_squared_error(y_test, predicted))\n",
    "    #     print(mean_squared_error(y_test, predicted))\n",
    "    avg_mse = np.mean(mses_per_layer)\n",
    "    print('Layer size {} MSE:\\n {}'.format(size, np.around(avg_mse, 2)))\n",
    "    \n",
    "    return avg_mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### No preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (439, 5)\n",
      "y shape: (439,)\n",
      "Layer size (50, 50) MSE:\n",
      " 98285603.12\n",
      "Layer size (100, 100) MSE:\n",
      " 12849881.22\n",
      "Layer size (100, 100, 100) MSE:\n",
      " 24222995.55\n",
      "Layer size (100, 100, 100, 100) MSE:\n",
      " 19350538.66\n",
      "Layer size (50, 50, 50, 50, 50, 50, 50, 50, 50, 50) MSE:\n",
      " 7834581.0\n",
      "Layer size (100, 100, 100, 100, 100, 100, 100, 100, 100, 100) MSE:\n",
      " 7180733.52\n"
     ]
    }
   ],
   "source": [
    "y_before = data_aggregate_before[1:,0]\n",
    "X_before = np.delete(data_aggregate_before, -1, 0)\n",
    "\n",
    "X = X_before\n",
    "y = y_before\n",
    "\n",
    "print('X shape:', X.shape)\n",
    "print('y shape:', y.shape)\n",
    "\n",
    "mses_before = []\n",
    "for size in layer_sizes:\n",
    "    nn = MLPRegressor(hidden_layer_sizes=size, activation='relu', solver='adam', alpha=0.001)\n",
    "\n",
    "    avg_mse = analyze_nn(nn, X, y, size)\n",
    "    mses_before.append(avg_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (143, 5)\n",
      "y shape: (143,)\n",
      "Layer size (50, 50) MSE:\n",
      " 18117620527339.48\n",
      "Layer size (100, 100) MSE:\n",
      " 102303823343.34\n",
      "Layer size (100, 100, 100) MSE:\n",
      " 486132967786.56\n",
      "Layer size (100, 100, 100, 100) MSE:\n",
      " 59044144675.32\n",
      "Layer size (50, 50, 50, 50, 50, 50, 50, 50, 50, 50) MSE:\n",
      " 183715723.0\n",
      "Layer size (100, 100, 100, 100, 100, 100, 100, 100, 100, 100) MSE:\n",
      " 176005234.96\n"
     ]
    }
   ],
   "source": [
    "y_during = data_aggregate_during[1:,0]\n",
    "X_during = np.delete(data_aggregate_during, -1, 0)\n",
    "\n",
    "X = X_during\n",
    "y = y_during\n",
    "\n",
    "print('X shape:', X.shape)\n",
    "print('y shape:', y.shape)\n",
    "\n",
    "mses_during = []\n",
    "for size in layer_sizes:\n",
    "    nn = MLPRegressor(hidden_layer_sizes=size, activation='relu', solver='adam', alpha=0.001)\n",
    "\n",
    "    avg_mse = analyze_nn(nn, X, y, size)\n",
    "    mses_during.append(avg_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (134, 5)\n",
      "y shape: (134,)\n",
      "Layer size (50, 50) MSE:\n",
      " 156130187198.86\n",
      "Layer size (100, 100) MSE:\n",
      " 11184102016.2\n",
      "Layer size (100, 100, 100) MSE:\n",
      " 36013255090.97\n",
      "Layer size (100, 100, 100, 100) MSE:\n",
      " 20519256630.1\n",
      "Layer size (50, 50, 50, 50, 50, 50, 50, 50, 50, 50) MSE:\n",
      " 5761544.24\n",
      "Layer size (100, 100, 100, 100, 100, 100, 100, 100, 100, 100) MSE:\n",
      " 17859847.26\n"
     ]
    }
   ],
   "source": [
    "y_after = data_aggregate_after[1:,0]\n",
    "X_after = np.delete(data_aggregate_after, -1, 0)\n",
    "\n",
    "X = X_after\n",
    "y = y_after\n",
    "\n",
    "print('X shape:', X.shape)\n",
    "print('y shape:', y.shape)\n",
    "\n",
    "mses_after = []\n",
    "for size in layer_sizes:\n",
    "    nn = MLPRegressor(hidden_layer_sizes=size, activation='relu', solver='adam', alpha=0.001)\n",
    "\n",
    "    avg_mse = analyze_nn(nn, X, y, size)\n",
    "    mses_after.append(avg_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9913.90957773 3584.67309839 4921.68625066 4398.92471675 2799.03215447\n",
      " 2679.68907204]\n",
      "[4256479.82813727  319849.68867163  697232.3628365   242990.00941463\n",
      "   13554.17732642   13266.6964598 ]\n",
      "[395133.12591943 105754.91485602 189771.58662711 143245.44191737\n",
      "   2400.32169447   4226.09125118]\n"
     ]
    }
   ],
   "source": [
    "print(np.sqrt(mses_before))\n",
    "print(np.sqrt(mses_during))\n",
    "print(np.sqrt(mses_after))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard scaler preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (439, 5)\n",
      "y shape: (439,)\n",
      "Layer size (50, 50) MSE:\n",
      " 6050233.73\n",
      "Layer size (100, 100) MSE:\n",
      " 5110956.82\n",
      "Layer size (100, 100, 100) MSE:\n",
      " 4751427.18\n",
      "Layer size (100, 100, 100, 100) MSE:\n",
      " 5020096.21\n",
      "Layer size (50, 50, 50, 50, 50, 50, 50, 50, 50, 50) MSE:\n",
      " 5819653.3\n",
      "Layer size (100, 100, 100, 100, 100, 100, 100, 100, 100, 100) MSE:\n",
      " 7057739.0\n"
     ]
    }
   ],
   "source": [
    "X = scaler.fit_transform(X_before)\n",
    "y = y_before\n",
    "\n",
    "print('X shape:', X.shape)\n",
    "print('y shape:', y.shape)\n",
    "\n",
    "mses_before = []\n",
    "for size in layer_sizes:\n",
    "    nn = MLPRegressor(hidden_layer_sizes=size, activation='relu', solver='adam', alpha=0.001)\n",
    "\n",
    "    avg_mse = analyze_nn(nn, X, y, size)\n",
    "    mses_before.append(avg_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (143, 5)\n",
      "y shape: (143,)\n",
      "Layer size (50, 50) MSE:\n",
      " 303747463.08\n",
      "Layer size (100, 100) MSE:\n",
      " 288660097.72\n",
      "Layer size (100, 100, 100) MSE:\n",
      " 80446124.47\n",
      "Layer size (100, 100, 100, 100) MSE:\n",
      " 52269559.79\n",
      "Layer size (50, 50, 50, 50, 50, 50, 50, 50, 50, 50) MSE:\n",
      " 27087474.1\n",
      "Layer size (100, 100, 100, 100, 100, 100, 100, 100, 100, 100) MSE:\n",
      " 27929247.19\n"
     ]
    }
   ],
   "source": [
    "X = scaler.fit_transform(X_during)\n",
    "y = y_during\n",
    "\n",
    "print('X shape:', X.shape)\n",
    "print('y shape:', y.shape)\n",
    "\n",
    "mses_during = []\n",
    "for size in layer_sizes:\n",
    "    nn = MLPRegressor(hidden_layer_sizes=size, activation='relu', solver='adam', alpha=0.001)\n",
    "\n",
    "    avg_mse = analyze_nn(nn, X, y, size)\n",
    "    mses_during.append(avg_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (134, 5)\n",
      "y shape: (134,)\n",
      "Layer size (50, 50) MSE:\n",
      " 5362373.3\n",
      "Layer size (100, 100) MSE:\n",
      " 3986058.6\n",
      "Layer size (100, 100, 100) MSE:\n",
      " 1768669.54\n",
      "Layer size (100, 100, 100, 100) MSE:\n",
      " 1441405.13\n",
      "Layer size (50, 50, 50, 50, 50, 50, 50, 50, 50, 50) MSE:\n",
      " 1078216.0\n",
      "Layer size (100, 100, 100, 100, 100, 100, 100, 100, 100, 100) MSE:\n",
      " 876148.24\n"
     ]
    }
   ],
   "source": [
    "X = scaler.fit_transform(X_after)\n",
    "y = y_after\n",
    "\n",
    "print('X shape:', X.shape)\n",
    "print('y shape:', y.shape)\n",
    "\n",
    "mses_after = []\n",
    "for size in layer_sizes:\n",
    "    nn = MLPRegressor(hidden_layer_sizes=size, activation='relu', solver='adam', alpha=0.001)\n",
    "\n",
    "    avg_mse = analyze_nn(nn, X, y, size)\n",
    "    mses_after.append(avg_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2459.72228665 2260.74253704 2179.77686376 2240.55712    2412.39575917\n",
      " 2656.64054704]\n",
      "[17428.35227655 16989.99993298  8969.17635395  7229.76899966\n",
      "  5204.56281528  5284.81288082]\n",
      "[2315.67987834 1996.51160876 1329.91335651 1200.58532616 1038.3718009\n",
      "  936.02790447]\n"
     ]
    }
   ],
   "source": [
    "print(np.sqrt(mses_before))\n",
    "print(np.sqrt(mses_during))\n",
    "print(np.sqrt(mses_after))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [anaconda]",
   "language": "python",
   "name": "Python [anaconda]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
