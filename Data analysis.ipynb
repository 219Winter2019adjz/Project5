{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import json\n",
    "from datetime import datetime\n",
    "%matplotlib inline\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "training_dir = os.path.join(\"Datasets\", \"Training\")\n",
    "testing_dir = os.path.join(\"Datasets\", \"Testing\")\n",
    "\n",
    "if not os.path.isdir(training_dir):\n",
    "    raise Exception(\"ERROR: training dataset not found\")\n",
    "\n",
    "if not os.path.isdir(testing_dir):\n",
    "    raise Exception(\"ERROR: testing dataset not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Analyzing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Size/RAM experiments: loading all training as lists requires approximately 10.3 GB of RAM. It is suggested to only extract the features you need each time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets\\Training\\tweets_#gohawks.txt\n",
      "Datasets\\Training\\tweets_#gopatriots.txt\n",
      "Datasets\\Training\\tweets_#nfl.txt\n",
      "Datasets\\Training\\tweets_#patriots.txt\n",
      "Datasets\\Training\\tweets_#sb49.txt\n",
      "Datasets\\Training\\tweets_#superbowl.txt\n"
     ]
    }
   ],
   "source": [
    "# iterate over all hashtag files \n",
    "for root, dirs, files in os.walk(training_dir, topdown=False):\n",
    "    for file in files:\n",
    "        print(os.path.join(root, file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get average statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing gohawks...\n",
      "\tnumber of tweets in period: 169122\n",
      "\tnumber of hours in period: 34693.13333333333\n",
      "\taverage tweets per hour: 4.874797510362281\n",
      "Parsing gopatriots...\n",
      "\tnumber of tweets in period: 23511\n",
      "\tnumber of hours in period: 34444.4\n",
      "\taverage tweets per hour: 0.6825783001010324\n",
      "Parsing nfl...\n",
      "\tnumber of tweets in period: 233022\n",
      "\tnumber of hours in period: 35215.53333333333\n",
      "\taverage tweets per hour: 6.617023169699735\n",
      "Parsing patriots...\n",
      "\tnumber of tweets in period: 440621\n",
      "\tnumber of hours in period: 35207.7\n",
      "\taverage tweets per hour: 12.514904410114834\n",
      "Parsing sb49...\n",
      "\tnumber of tweets in period: 743649\n",
      "\tnumber of hours in period: 34944.35\n",
      "\taverage tweets per hour: 21.28095099780079\n",
      "Parsing superbowl...\n",
      "\tnumber of tweets in period: 1213813\n",
      "\tnumber of hours in period: 35147.01666666667\n",
      "\taverage tweets per hour: 34.535306695068\n"
     ]
    }
   ],
   "source": [
    "# iterate over all hashtag files \n",
    "for root, dirs, files in os.walk(training_dir, topdown=False):\n",
    "    for file in files:\n",
    "        # the name of the key\n",
    "        filename = os.path.splitext(file)[0].replace('tweets_#', '')\n",
    "        print('Parsing {}...'.format(filename))\n",
    "        \n",
    "        # only extracting times from the tweet json objects\n",
    "        citation_dates = []\n",
    "        \n",
    "        # open the file and read all lines:\n",
    "        with open(os.path.join(root, file), \"r\", encoding=\"utf-8\") as hashtag:\n",
    "            # read line-by-line\n",
    "            for line in hashtag:\n",
    "                json_obj = json.loads(line)\n",
    "                citation_date = json_obj['citation_date']\n",
    "                citation_dates.append(citation_date)\n",
    "                \n",
    "        citation_dates = np.array(citation_dates)\n",
    "        print('\\tnumber of tweets in period: {}'.format(len(citation_dates)))\n",
    "        min_date = np.min(citation_dates)\n",
    "        max_date = np.max(citation_dates)\n",
    "        \n",
    "        span_hours = (max_date - min_date)/60\n",
    "        print('\\tnumber of hours in period: {}'.format(span_hours))\n",
    "        \n",
    "        tweets_per_hour = len(citation_dates) / span_hours\n",
    "        print('\\taverage tweets per hour: {}'.format(tweets_per_hour))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
