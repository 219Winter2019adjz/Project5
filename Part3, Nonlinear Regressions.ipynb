{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import json\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import pytz\n",
    "%matplotlib inline\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and Testing directories\n",
    "training_dir = os.path.join(\"Datasets\", \"Training\")\n",
    "testing_dir = os.path.join(\"Datasets\", \"Testing\")\n",
    "if not os.path.isdir(training_dir):\n",
    "    raise Exception(\"ERROR: training dataset not found\")\n",
    "if not os.path.isdir(testing_dir):\n",
    "    raise Exception(\"ERROR: testing dataset not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets\\Training\\tweets_#gohawks.txt\n",
      "Datasets\\Training\\tweets_#gopatriots.txt\n",
      "Datasets\\Training\\tweets_#nfl.txt\n",
      "Datasets\\Training\\tweets_#patriots.txt\n",
      "Datasets\\Training\\tweets_#sb49.txt\n",
      "Datasets\\Training\\tweets_#superbowl.txt\n"
     ]
    }
   ],
   "source": [
    "# iterate over all hashtag files \n",
    "for root, dirs, files in os.walk(training_dir, topdown=False):\n",
    "    for file in files:\n",
    "        print(os.path.join(root, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dictionaries and Unix times for Feb 1, 8 am and Feb 1, 8 pm. \n",
    "# Dictionary keys: hashtag.\n",
    "# Dictionary values: [time of tweet (Unix), number of retweets for tweet, number of followers for tweeter]\n",
    "# Each row in dictionary value is an individual tweet.\n",
    "\n",
    "hashtag_dict_before = {}\n",
    "hashtag_dict_during = {}\n",
    "hashtag_dict_after = {}\n",
    "start_unix_time = 1422806400 # 8 am, Feb 1, PST\n",
    "end_unix_time = 1422849600 # 8 pm, Feb 1, PST\n",
    "pst_tz = pytz.timezone('America/Los_Angeles')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing gohawks...\n",
      "Parsing gopatriots...\n",
      "Parsing nfl...\n",
      "Parsing patriots...\n",
      "Parsing sb49...\n",
      "Parsing superbowl...\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "for root, dirs, files in os.walk(training_dir, topdown=False):\n",
    "    for file in files:\n",
    "        filename = os.path.splitext(file)[0].replace('tweets_#', '')\n",
    "        print('Parsing {}...'.format(filename))\n",
    "        \n",
    "        hashtag_dict_before[filename] = []\n",
    "        hashtag_dict_during[filename] = []\n",
    "        hashtag_dict_after[filename] = []\n",
    "        \n",
    "        # open the file and read all lines:\n",
    "        with open(os.path.join(root, file), \"r\", encoding=\"utf-8\") as hashtag:\n",
    "            # read line-by-line\n",
    "            for line in hashtag:\n",
    "                json_obj = json.loads(line)\n",
    "                \n",
    "                # get desired statistics\n",
    "                citation_date = json_obj['citation_date'] # Unix time\n",
    "                num_retweets = json_obj['metrics']['citations']['total'] # Number of retweets for this tweet\n",
    "                num_followers = json_obj['author']['followers'] # Number of followers for tweeter\n",
    "                \n",
    "                # Check when tweet was made and add it to corresponding dictionary\n",
    "                if citation_date < start_unix_time:\n",
    "                    hashtag_dict_before[filename].append([citation_date, num_retweets, num_followers])\n",
    "                elif citation_date > end_unix_time:\n",
    "                    hashtag_dict_after[filename].append([citation_date, num_retweets, num_followers])\n",
    "                else:\n",
    "                    hashtag_dict_during[filename].append([citation_date, num_retweets, num_followers])\n",
    "    print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explicitly list hashtags. \n",
    "# Convert each value in dictionary to numpy arrays.\n",
    "\n",
    "hashtags = ['gohawks', 'gopatriots', 'nfl', 'patriots', 'sb49', 'superbowl']\n",
    "\n",
    "for key in hashtags:\n",
    "    hashtag_dict_before[key] = np.array(hashtag_dict_before[key])\n",
    "    hashtag_dict_during[key] = np.array(hashtag_dict_during[key])\n",
    "    hashtag_dict_after[key] = np.array(hashtag_dict_after[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find how many time windows there are\n",
    "\n",
    "ftt = int(np.min([np.min(hashtag_dict_before[key][:,0]) for key in hashtags])) # first tweet time\n",
    "ltt = int(np.max([np.max(hashtag_dict_after[key][:,0]) for key in hashtags])) # last tweet time\n",
    "\n",
    "num_windows_before = int(np.max([((start_unix_time - ftt) // 3600) + 1 for key in hashtags]))\n",
    "num_windows_during = int(np.max([((end_unix_time - start_unix_time) // 3600 * 12) for key in hashtags]))\n",
    "num_windows_after = int(np.max([((ltt - end_unix_time) // 3600) + 1 for key in hashtags]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gohawks\n",
      "gopatriots\n",
      "nfl\n",
      "patriots\n",
      "sb49\n",
      "superbowl\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# Organize data into specific time frames.\n",
    "\n",
    "# Initialize dictionary for each time frame.\n",
    "data_hashtag_before = {}\n",
    "data_hashtag_during = {}\n",
    "data_hashtag_after = {}\n",
    "\n",
    "# Iterate through each hashtag.\n",
    "for key in hashtags:\n",
    "    print(key)\n",
    "    \n",
    "    # Rename the dictionary value for readability\n",
    "    temp_before = hashtag_dict_before[key]\n",
    "    temp_during = hashtag_dict_during[key]\n",
    "    temp_after = hashtag_dict_after[key]\n",
    "    \n",
    "    data_hashtag_before[key] = np.zeros((num_windows_before, 5)) # Initialize array: rows = time window, columns = feature\n",
    "    num_followers_before = {} # Initialize dictionary to count # of followers for each tweet\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Iterate through all elements before start time\n",
    "    for i in range(np.shape(temp_before)[0]):\n",
    "        # Get row number\n",
    "        item_before = int(num_windows_before - 1 - ((start_unix_time - temp_before[i,0] - 1) // 3600))\n",
    "        # Update first 3 elements (# of tweets, total # retweets, total # followers)\n",
    "        data_hashtag_before[key][item_before] += np.array([1, int(temp_before[i, 1]), int(temp_before[i, 2]), 0, 0])\n",
    "        # Get time of day (hour)\n",
    "        dt_obj_pst = datetime.fromtimestamp(temp_before[i,0], pst_tz)\n",
    "        data_hashtag_before[key][item_before][4] = int(datetime.strftime(dt_obj_pst, '%H'))\n",
    "        # Get number of followers\n",
    "        if item_before not in num_followers_before.keys():\n",
    "            num_followers_before[item_before] = []\n",
    "        num_followers_before[item_before].append(temp_before[i,2])\n",
    "    for i in num_followers_before.keys():\n",
    "        data_hashtag_before[key][i][3] = np.max(num_followers_before[i])\n",
    "        \n",
    "        \n",
    "    # Iterate through all elements during time\n",
    "    data_hashtag_during[key] = np.zeros((num_windows_during, 5))\n",
    "    num_followers_during = {}\n",
    "    for i in range(np.shape(temp_during)[0]):\n",
    "        item_during = int(((temp_during[i,0] - start_unix_time) * 12) // 3600)\n",
    "        data_hashtag_during[key][item_during] += np.array([1, int(temp_during[i, 1]), int(temp_during[i, 2]), 0, 0])\n",
    "        dt_obj_pst = datetime.fromtimestamp(temp_during[i,0], pst_tz)\n",
    "        data_hashtag_during[key][item_during][4] = int(datetime.strftime(dt_obj_pst, '%H'))\n",
    "        \n",
    "        if item_during not in num_followers_during.keys():\n",
    "            num_followers_during[item_during] = []\n",
    "        num_followers_during[item_during].append(temp_during[i,2])\n",
    "    for i in num_followers_during.keys():\n",
    "        data_hashtag_during[key][i][3] = np.max(num_followers_during[i])\n",
    "        \n",
    "    # Iterate through all elements after end time\n",
    "    data_hashtag_after[key] = np.zeros((num_windows_after, 5))\n",
    "    num_followers_after = {}\n",
    "    for i in range(np.shape(temp_after)[0]):\n",
    "        item_after = int((temp_after[i,0] - end_unix_time) // 3600)\n",
    "        data_hashtag_after[key][item_after] += np.array([1, int(temp_after[i, 1]), int(temp_after[i, 2]), 0, 0])\n",
    "        dt_obj_pst = datetime.fromtimestamp(temp_after[i,0], pst_tz)\n",
    "        data_hashtag_after[key][item_after][4] = int(datetime.strftime(dt_obj_pst, '%H'))\n",
    "        \n",
    "        if item_after not in num_followers_after.keys():\n",
    "            num_followers_after[item_after] = []\n",
    "        num_followers_after[item_after].append(temp_after[i,2])\n",
    "    for i in num_followers_after.keys():\n",
    "        data_hashtag_after[key][i][3] = np.max(num_followers_after[i])\n",
    "        \n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate data\n",
    "\n",
    "# Initialize aggregated data variables\n",
    "data_aggregate_before = np.zeros([num_windows_before, 5])\n",
    "data_aggregate_during = np.zeros([num_windows_during, 5])\n",
    "data_aggregate_after = np.zeros([num_windows_after, 5])\n",
    "\n",
    "# Sum the # of tweets, total # of retweets, and # of followers\n",
    "for key in hashtags:\n",
    "    data_aggregate_before[:,0:3] += data_hashtag_before[key][:,0:3]\n",
    "    data_aggregate_during[:,0:3] += data_hashtag_during[key][:,0:3]\n",
    "    data_aggregate_after[:,0:3] += data_hashtag_after[key][:,0:3]\n",
    "# Find the max # of followers for each\n",
    "data_aggregate_before[:,3] = np.amax([data_hashtag_before[key][:,3] for key in hashtags], axis=0)\n",
    "data_aggregate_during[:,3] = np.amax([data_hashtag_during[key][:,3] for key in hashtags], axis=0)\n",
    "data_aggregate_after[:,3] = np.amax([data_hashtag_after[key][:,3] for key in hashtags], axis=0)\n",
    "\n",
    "# Copy over the same time frames\n",
    "data_aggregate_before[:,4] = data_hashtag_before['superbowl'][:,4]\n",
    "data_aggregate_during[:,4] = data_hashtag_during['superbowl'][:,4]\n",
    "data_aggregate_after[:,4] = data_hashtag_after['superbowl'][:,4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nonlinear Regressions: Ensemble methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform GridSearch\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "import time\n",
    "\n",
    "param_grid = {\n",
    "    'max_depth': [10, 20, 40, 60, 80, 100, 200, None],\n",
    "    'max_features': ['auto', 'sqrt'],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_results(grid_search_cv, top_results=15):\n",
    "    print(\"Top {} grid search scores on the basis of mean validation accuracy: \".format(top_results))\n",
    "    print()\n",
    "    means = grid_search_cv.cv_results_['mean_test_score']\n",
    "    stds = grid_search_cv.cv_results_['std_test_score']\n",
    "    params = grid_search_cv.cv_results_['params']\n",
    "    \n",
    "    np_rep = np.array([means, stds, params])\n",
    "    np_rep = np_rep.T\n",
    "    \n",
    "    # sort this array\n",
    "    sorted_gs = np_rep[(-np_rep[:,0]).argsort()]\n",
    "    \n",
    "    for i in range(top_results):\n",
    "        mean, std, param = sorted_gs[i]\n",
    "        print(\"%0.6f (+/-%0.06f) for %r\" % (mean, std * 2, param))\n",
    "    print()\n",
    "\n",
    "    print(\"Optimal value of C: \")\n",
    "    print()\n",
    "    print(grid_search_cv.best_params_)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Analysis of data aggregated before Feb 1, 8:00 am"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (439, 5)\n",
      "y shape: (439,)\n"
     ]
    }
   ],
   "source": [
    "# Define train data and targets for BEFORE period\n",
    "y_before = data_aggregate_before[1:,0] # Number of tweets (except first)\n",
    "X_before = np.delete(data_aggregate_before, -1, 0) # Delete last row\n",
    "\n",
    "print('X shape:', X_before.shape)\n",
    "print('y shape:', y_before.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "performing RF grid search...\n",
      "Fitting 5 folds for each of 1440 candidates, totalling 7200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:   16.7s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=4)]: Done 792 tasks      | elapsed:  4.5min\n",
      "[Parallel(n_jobs=4)]: Done 1242 tasks      | elapsed:  7.1min\n",
      "[Parallel(n_jobs=4)]: Done 1792 tasks      | elapsed: 10.3min\n",
      "[Parallel(n_jobs=4)]: Done 2442 tasks      | elapsed: 14.2min\n",
      "[Parallel(n_jobs=4)]: Done 3192 tasks      | elapsed: 18.9min\n",
      "[Parallel(n_jobs=4)]: Done 4042 tasks      | elapsed: 23.8min\n",
      "[Parallel(n_jobs=4)]: Done 4992 tasks      | elapsed: 29.4min\n",
      "[Parallel(n_jobs=4)]: Done 6042 tasks      | elapsed: 35.4min\n",
      "[Parallel(n_jobs=4)]: Done 7192 tasks      | elapsed: 41.9min\n",
      "[Parallel(n_jobs=4)]: Done 7200 out of 7200 | elapsed: 42.0min finished\n",
      "C:\\Users\\zharr\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grid search took 2521.66 seconds\n"
     ]
    }
   ],
   "source": [
    "# Random Forest regressor grid search\n",
    "print('performing RF grid search...')\n",
    "\n",
    "rf_cv_bef = GridSearchCV(RandomForestRegressor(), param_grid, cv=KFold(5, shuffle=True), scoring='neg_mean_squared_error',\n",
    "                         verbose=1, n_jobs=4)\n",
    "rf_cv_bef.fit(X_before, y_before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 15 grid search scores on the basis of mean validation accuracy: \n",
      "\n",
      "-3796709.438480 (+/-9260728.938884) for {'min_samples_leaf': 4, 'max_features': 'sqrt', 'max_depth': 10, 'n_estimators': 200, 'min_samples_split': 2}\n",
      "-3798615.766410 (+/-8101520.055596) for {'min_samples_leaf': 4, 'max_features': 'auto', 'max_depth': 80, 'n_estimators': 800, 'min_samples_split': 10}\n",
      "-3801999.676896 (+/-8149097.663559) for {'min_samples_leaf': 4, 'max_features': 'auto', 'max_depth': 80, 'n_estimators': 600, 'min_samples_split': 10}\n",
      "-3806065.756227 (+/-9416810.046392) for {'min_samples_leaf': 4, 'max_features': 'sqrt', 'max_depth': 20, 'n_estimators': 600, 'min_samples_split': 5}\n",
      "-3807089.707171 (+/-9319740.786582) for {'min_samples_leaf': 4, 'max_features': 'sqrt', 'max_depth': 100, 'n_estimators': 1000, 'min_samples_split': 5}\n",
      "-3812964.958369 (+/-8141324.618744) for {'min_samples_leaf': 4, 'max_features': 'auto', 'max_depth': None, 'n_estimators': 1600, 'min_samples_split': 10}\n",
      "-3814777.875293 (+/-9202134.741742) for {'min_samples_leaf': 4, 'max_features': 'sqrt', 'max_depth': 60, 'n_estimators': 200, 'min_samples_split': 2}\n",
      "-3816661.491763 (+/-8149619.366592) for {'min_samples_leaf': 4, 'max_features': 'auto', 'max_depth': 20, 'n_estimators': 800, 'min_samples_split': 10}\n",
      "-3817828.466409 (+/-8265946.076321) for {'min_samples_leaf': 4, 'max_features': 'auto', 'max_depth': 10, 'n_estimators': 200, 'min_samples_split': 10}\n",
      "-3818093.808954 (+/-8142951.312901) for {'min_samples_leaf': 4, 'max_features': 'auto', 'max_depth': 100, 'n_estimators': 2000, 'min_samples_split': 10}\n",
      "-3818594.866659 (+/-8151349.708751) for {'min_samples_leaf': 4, 'max_features': 'auto', 'max_depth': 20, 'n_estimators': 200, 'min_samples_split': 10}\n",
      "-3823166.544898 (+/-8127000.421128) for {'min_samples_leaf': 4, 'max_features': 'auto', 'max_depth': 100, 'n_estimators': 1400, 'min_samples_split': 10}\n",
      "-3823604.829804 (+/-9501976.758181) for {'min_samples_leaf': 4, 'max_features': 'sqrt', 'max_depth': None, 'n_estimators': 200, 'min_samples_split': 10}\n",
      "-3823655.817044 (+/-9391165.110027) for {'min_samples_leaf': 4, 'max_features': 'sqrt', 'max_depth': 80, 'n_estimators': 1400, 'min_samples_split': 5}\n",
      "-3824042.708535 (+/-8208637.127874) for {'min_samples_leaf': 4, 'max_features': 'auto', 'max_depth': 80, 'n_estimators': 2000, 'min_samples_split': 10}\n",
      "\n",
      "Optimal value of C: \n",
      "\n",
      "{'min_samples_leaf': 4, 'max_features': 'sqrt', 'max_depth': 10, 'n_estimators': 200, 'min_samples_split': 2}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "show_results(rf_cv_bef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "performing GB grid search...\n",
      "Fitting 5 folds for each of 1440 candidates, totalling 7200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    4.2s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:   18.6s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:   47.7s\n",
      "[Parallel(n_jobs=4)]: Done 792 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=4)]: Done 1242 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=4)]: Done 1792 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=4)]: Done 2442 tasks      | elapsed:  4.3min\n",
      "[Parallel(n_jobs=4)]: Done 3192 tasks      | elapsed:  5.8min\n",
      "[Parallel(n_jobs=4)]: Done 4042 tasks      | elapsed:  7.4min\n",
      "[Parallel(n_jobs=4)]: Done 4992 tasks      | elapsed:  9.2min\n",
      "[Parallel(n_jobs=4)]: Done 6042 tasks      | elapsed: 11.0min\n",
      "[Parallel(n_jobs=4)]: Done 7192 tasks      | elapsed: 13.2min\n",
      "[Parallel(n_jobs=4)]: Done 7200 out of 7200 | elapsed: 13.2min finished\n",
      "C:\\Users\\zharr\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=KFold(n_splits=5, random_state=None, shuffle=True),\n",
       "       error_score='raise-deprecating',\n",
       "       estimator=GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.1, loss='ls', max_depth=3, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_sampl...=None, subsample=1.0, tol=0.0001,\n",
       "             validation_fraction=0.1, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid='warn', n_jobs=4,\n",
       "       param_grid={'min_samples_leaf': [1, 2, 4], 'max_features': ['auto', 'sqrt'], 'max_depth': [10, 20, 40, 60, 80, 100, 200, None], 'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000], 'min_samples_split': [2, 5, 10]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='neg_mean_squared_error', verbose=1)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GradientBoostingRegressor regressor grid search\n",
    "print('performing GB grid search...')\n",
    "\n",
    "gb_cv_bef = GridSearchCV(GradientBoostingRegressor(), param_grid, cv=KFold(5, shuffle=True), scoring='neg_mean_squared_error',\n",
    "                         verbose=1, n_jobs=4)\n",
    "gb_cv_bef.fit(X_before, y_before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 15 grid search scores on the basis of mean validation accuracy: \n",
      "\n",
      "-4547351.762900 (+/-8608540.176647) for {'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 100, 'n_estimators': 1400, 'min_samples_split': 2}\n",
      "-4744032.427315 (+/-8974336.689770) for {'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 20, 'n_estimators': 400, 'min_samples_split': 2}\n",
      "-4752388.329574 (+/-9040748.516853) for {'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 80, 'n_estimators': 400, 'min_samples_split': 2}\n",
      "-4768935.994495 (+/-9300204.802032) for {'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 60, 'n_estimators': 800, 'min_samples_split': 5}\n",
      "-4774098.289003 (+/-9241963.989489) for {'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 40, 'n_estimators': 1000, 'min_samples_split': 2}\n",
      "-4792073.917406 (+/-9094682.522391) for {'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 80, 'n_estimators': 1600, 'min_samples_split': 10}\n",
      "-4824671.230657 (+/-8866720.900590) for {'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 80, 'n_estimators': 1000, 'min_samples_split': 2}\n",
      "-4850292.786894 (+/-8746196.095038) for {'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 60, 'n_estimators': 1400, 'min_samples_split': 2}\n",
      "-4863137.105921 (+/-8810361.402496) for {'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 200, 'n_estimators': 1600, 'min_samples_split': 2}\n",
      "-4888451.696662 (+/-9110739.866711) for {'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 100, 'n_estimators': 200, 'min_samples_split': 2}\n",
      "-4892513.482993 (+/-9225883.189274) for {'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 200, 'n_estimators': 2000, 'min_samples_split': 10}\n",
      "-4897662.395404 (+/-9574226.936888) for {'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': None, 'n_estimators': 400, 'min_samples_split': 2}\n",
      "-4914893.873473 (+/-8904527.645963) for {'min_samples_leaf': 2, 'max_features': 'sqrt', 'max_depth': 80, 'n_estimators': 1600, 'min_samples_split': 10}\n",
      "-4923390.315031 (+/-8822551.455286) for {'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 10, 'n_estimators': 1600, 'min_samples_split': 2}\n",
      "-4923713.558220 (+/-8608070.513563) for {'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 60, 'n_estimators': 1200, 'min_samples_split': 2}\n",
      "\n",
      "Optimal value of C: \n",
      "\n",
      "{'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 100, 'n_estimators': 1400, 'min_samples_split': 2}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "show_results(gb_cv_bef)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Analysis of data aggregated between Feb 1, 8:00 am and Feb 1, 8:00 pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (143, 5)\n",
      "y shape: (143,)\n"
     ]
    }
   ],
   "source": [
    "y_during = data_aggregate_during[1:,0]\n",
    "X_during = np.delete(data_aggregate_during, -1, 0)\n",
    "\n",
    "print('X shape:', X_during.shape)\n",
    "print('y shape:', y_during.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "performing RF grid search...\n",
      "Fitting 5 folds for each of 1440 candidates, totalling 7200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:   12.1s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:   57.1s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=4)]: Done 792 tasks      | elapsed:  3.9min\n",
      "[Parallel(n_jobs=4)]: Done 1242 tasks      | elapsed:  6.1min\n",
      "[Parallel(n_jobs=4)]: Done 1792 tasks      | elapsed:  8.9min\n",
      "[Parallel(n_jobs=4)]: Done 2442 tasks      | elapsed: 12.1min\n",
      "[Parallel(n_jobs=4)]: Done 3192 tasks      | elapsed: 15.8min\n",
      "[Parallel(n_jobs=4)]: Done 4042 tasks      | elapsed: 20.0min\n",
      "[Parallel(n_jobs=4)]: Done 4992 tasks      | elapsed: 24.7min\n",
      "[Parallel(n_jobs=4)]: Done 6042 tasks      | elapsed: 30.0min\n",
      "[Parallel(n_jobs=4)]: Done 7192 tasks      | elapsed: 35.7min\n",
      "[Parallel(n_jobs=4)]: Done 7200 out of 7200 | elapsed: 35.7min finished\n",
      "C:\\Users\\zharr\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=KFold(n_splits=5, random_state=None, shuffle=True),\n",
       "       error_score='raise-deprecating',\n",
       "       estimator=RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None,\n",
       "           oob_score=False, random_state=None, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid='warn', n_jobs=4,\n",
       "       param_grid={'min_samples_leaf': [1, 2, 4], 'max_features': ['auto', 'sqrt'], 'max_depth': [10, 20, 40, 60, 80, 100, 200, None], 'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000], 'min_samples_split': [2, 5, 10]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='neg_mean_squared_error', verbose=1)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random Forest regressor grid search\n",
    "print('performing RF grid search...')\n",
    "\n",
    "rf_cv_dur = GridSearchCV(RandomForestRegressor(), param_grid, cv=KFold(5, shuffle=True), scoring='neg_mean_squared_error',\n",
    "                         verbose=1, n_jobs=4)\n",
    "rf_cv_dur.fit(X_during, y_during)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 15 grid search scores on the basis of mean validation accuracy: \n",
      "\n",
      "-22375832.235636 (+/-18126868.166056) for {'min_samples_leaf': 4, 'max_features': 'sqrt', 'max_depth': 10, 'n_estimators': 200, 'min_samples_split': 10}\n",
      "-22402504.692351 (+/-17809279.100428) for {'min_samples_leaf': 4, 'max_features': 'sqrt', 'max_depth': 10, 'n_estimators': 800, 'min_samples_split': 10}\n",
      "-22437809.501365 (+/-18625753.436627) for {'min_samples_leaf': 4, 'max_features': 'sqrt', 'max_depth': 60, 'n_estimators': 400, 'min_samples_split': 10}\n",
      "-22459492.788931 (+/-18167911.195121) for {'min_samples_leaf': 4, 'max_features': 'sqrt', 'max_depth': None, 'n_estimators': 1000, 'min_samples_split': 10}\n",
      "-22495810.533135 (+/-17650250.880146) for {'min_samples_leaf': 4, 'max_features': 'sqrt', 'max_depth': 200, 'n_estimators': 200, 'min_samples_split': 10}\n",
      "-22524647.640517 (+/-18239702.041868) for {'min_samples_leaf': 4, 'max_features': 'sqrt', 'max_depth': 20, 'n_estimators': 200, 'min_samples_split': 2}\n",
      "-22533590.291685 (+/-17763934.996194) for {'min_samples_leaf': 4, 'max_features': 'sqrt', 'max_depth': 40, 'n_estimators': 400, 'min_samples_split': 10}\n",
      "-22540505.826156 (+/-17509026.342960) for {'min_samples_leaf': 4, 'max_features': 'sqrt', 'max_depth': None, 'n_estimators': 200, 'min_samples_split': 2}\n",
      "-22547931.624645 (+/-18644199.650138) for {'min_samples_leaf': 4, 'max_features': 'sqrt', 'max_depth': 200, 'n_estimators': 1800, 'min_samples_split': 10}\n",
      "-22549852.169897 (+/-18117555.222263) for {'min_samples_leaf': 4, 'max_features': 'sqrt', 'max_depth': 60, 'n_estimators': 800, 'min_samples_split': 10}\n",
      "-22553799.939157 (+/-12911643.844397) for {'min_samples_leaf': 1, 'max_features': 'auto', 'max_depth': 10, 'n_estimators': 200, 'min_samples_split': 10}\n",
      "-22563299.068663 (+/-18056120.888974) for {'min_samples_leaf': 4, 'max_features': 'sqrt', 'max_depth': 10, 'n_estimators': 1000, 'min_samples_split': 10}\n",
      "-22565525.689803 (+/-17615116.410273) for {'min_samples_leaf': 4, 'max_features': 'sqrt', 'max_depth': 10, 'n_estimators': 600, 'min_samples_split': 5}\n",
      "-22567671.228423 (+/-17965668.818653) for {'min_samples_leaf': 4, 'max_features': 'sqrt', 'max_depth': 100, 'n_estimators': 1600, 'min_samples_split': 2}\n",
      "-22575107.976550 (+/-17995245.195723) for {'min_samples_leaf': 4, 'max_features': 'sqrt', 'max_depth': 200, 'n_estimators': 400, 'min_samples_split': 5}\n",
      "\n",
      "Optimal value of C: \n",
      "\n",
      "{'min_samples_leaf': 4, 'max_features': 'sqrt', 'max_depth': 10, 'n_estimators': 200, 'min_samples_split': 10}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "show_results(rf_cv_dur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "performing GB grid search...\n",
      "Fitting 5 folds for each of 1440 candidates, totalling 7200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done 170 tasks      | elapsed:    7.8s\n",
      "[Parallel(n_jobs=4)]: Done 620 tasks      | elapsed:   32.5s\n",
      "[Parallel(n_jobs=4)]: Done 1344 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=4)]: Done 1694 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=4)]: Done 2144 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=4)]: Done 2694 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=4)]: Done 3344 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=4)]: Done 4094 tasks      | elapsed:  3.9min\n",
      "[Parallel(n_jobs=4)]: Done 4944 tasks      | elapsed:  4.7min\n",
      "[Parallel(n_jobs=4)]: Done 5894 tasks      | elapsed:  5.6min\n",
      "[Parallel(n_jobs=4)]: Done 6944 tasks      | elapsed:  6.6min\n",
      "[Parallel(n_jobs=4)]: Done 7200 out of 7200 | elapsed:  6.9min finished\n",
      "C:\\Users\\zharr\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=KFold(n_splits=5, random_state=None, shuffle=True),\n",
       "       error_score='raise-deprecating',\n",
       "       estimator=GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.1, loss='ls', max_depth=3, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_sampl...=None, subsample=1.0, tol=0.0001,\n",
       "             validation_fraction=0.1, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid='warn', n_jobs=4,\n",
       "       param_grid={'min_samples_leaf': [1, 2, 4], 'max_features': ['auto', 'sqrt'], 'max_depth': [10, 20, 40, 60, 80, 100, 200, None], 'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000], 'min_samples_split': [2, 5, 10]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='neg_mean_squared_error', verbose=1)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GradientBoostingRegressor regressor grid search\n",
    "print('performing GB grid search...')\n",
    "\n",
    "gb_cv_dur = GridSearchCV(GradientBoostingRegressor(), param_grid, cv=KFold(5, shuffle=True), scoring='neg_mean_squared_error',\n",
    "                         verbose=1, n_jobs=4)\n",
    "gb_cv_dur.fit(X_during, y_during)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 15 grid search scores on the basis of mean validation accuracy: \n",
      "\n",
      "-26016035.436223 (+/-29443759.973838) for {'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 100, 'n_estimators': 1600, 'min_samples_split': 5}\n",
      "-26017401.691761 (+/-29074803.376647) for {'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 80, 'n_estimators': 1000, 'min_samples_split': 2}\n",
      "-26910814.492104 (+/-32117712.372015) for {'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 40, 'n_estimators': 800, 'min_samples_split': 2}\n",
      "-27086756.140145 (+/-26591957.649186) for {'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': None, 'n_estimators': 1000, 'min_samples_split': 5}\n",
      "-27329860.347148 (+/-30427611.225030) for {'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 60, 'n_estimators': 600, 'min_samples_split': 2}\n",
      "-27410617.355700 (+/-32148399.040868) for {'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 100, 'n_estimators': 1800, 'min_samples_split': 2}\n",
      "-27440592.748651 (+/-25615328.683425) for {'min_samples_leaf': 2, 'max_features': 'sqrt', 'max_depth': None, 'n_estimators': 600, 'min_samples_split': 5}\n",
      "-27556883.220546 (+/-29959720.606117) for {'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 60, 'n_estimators': 1400, 'min_samples_split': 2}\n",
      "-27561459.833661 (+/-27679385.384172) for {'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 20, 'n_estimators': 1200, 'min_samples_split': 5}\n",
      "-27598857.765708 (+/-28873780.575637) for {'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 40, 'n_estimators': 600, 'min_samples_split': 5}\n",
      "-27740276.436225 (+/-27393433.093791) for {'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': None, 'n_estimators': 1000, 'min_samples_split': 2}\n",
      "-27825845.587667 (+/-27425374.459056) for {'min_samples_leaf': 2, 'max_features': 'sqrt', 'max_depth': 10, 'n_estimators': 1200, 'min_samples_split': 2}\n",
      "-27845457.622998 (+/-29282038.135109) for {'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': None, 'n_estimators': 200, 'min_samples_split': 5}\n",
      "-27858110.784250 (+/-28456693.777206) for {'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 10, 'n_estimators': 2000, 'min_samples_split': 2}\n",
      "-27880250.586060 (+/-29080802.354253) for {'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 10, 'n_estimators': 1000, 'min_samples_split': 5}\n",
      "\n",
      "Optimal value of C: \n",
      "\n",
      "{'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 100, 'n_estimators': 1600, 'min_samples_split': 5}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "show_results(gb_cv_dur)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Analysis of data aggregated after Feb 1, 8:00 pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (134, 5)\n",
      "y shape: (134,)\n"
     ]
    }
   ],
   "source": [
    "y_after = data_aggregate_after[1:,0]\n",
    "X_after = np.delete(data_aggregate_after, -1, 0)\n",
    "\n",
    "print('X shape:', X_after.shape)\n",
    "print('y shape:', y_after.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "performing RF grid search...\n",
      "Fitting 5 folds for each of 1440 candidates, totalling 7200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:   13.2s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=4)]: Done 792 tasks      | elapsed:  4.3min\n",
      "[Parallel(n_jobs=4)]: Done 1242 tasks      | elapsed:  6.8min\n",
      "[Parallel(n_jobs=4)]: Done 1792 tasks      | elapsed:  9.7min\n",
      "[Parallel(n_jobs=4)]: Done 2442 tasks      | elapsed: 13.3min\n",
      "[Parallel(n_jobs=4)]: Done 3192 tasks      | elapsed: 17.3min\n",
      "[Parallel(n_jobs=4)]: Done 4042 tasks      | elapsed: 21.6min\n",
      "[Parallel(n_jobs=4)]: Done 4992 tasks      | elapsed: 26.3min\n",
      "[Parallel(n_jobs=4)]: Done 6042 tasks      | elapsed: 31.4min\n",
      "[Parallel(n_jobs=4)]: Done 7192 tasks      | elapsed: 37.0min\n",
      "[Parallel(n_jobs=4)]: Done 7200 out of 7200 | elapsed: 37.1min finished\n",
      "C:\\Users\\zharr\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=KFold(n_splits=5, random_state=None, shuffle=True),\n",
       "       error_score='raise-deprecating',\n",
       "       estimator=RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None,\n",
       "           oob_score=False, random_state=None, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid='warn', n_jobs=4,\n",
       "       param_grid={'min_samples_leaf': [1, 2, 4], 'max_features': ['auto', 'sqrt'], 'max_depth': [10, 20, 40, 60, 80, 100, 200, None], 'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000], 'min_samples_split': [2, 5, 10]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='neg_mean_squared_error', verbose=1)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random Forest regressor grid search\n",
    "print('performing RF grid search...')\n",
    "\n",
    "rf_cv_aft = GridSearchCV(RandomForestRegressor(), param_grid, cv=KFold(5, shuffle=True), scoring='neg_mean_squared_error',\n",
    "                         verbose=1, n_jobs=4)\n",
    "rf_cv_aft.fit(X_after, y_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 15 grid search scores on the basis of mean validation accuracy: \n",
      "\n",
      "-333882.199840 (+/-458509.946463) for {'min_samples_leaf': 2, 'max_features': 'auto', 'max_depth': 20, 'n_estimators': 600, 'min_samples_split': 5}\n",
      "-334992.322405 (+/-458854.509039) for {'min_samples_leaf': 2, 'max_features': 'auto', 'max_depth': 100, 'n_estimators': 400, 'min_samples_split': 5}\n",
      "-335075.909955 (+/-458333.138494) for {'min_samples_leaf': 2, 'max_features': 'auto', 'max_depth': 60, 'n_estimators': 800, 'min_samples_split': 2}\n",
      "-335243.886419 (+/-450289.921372) for {'min_samples_leaf': 2, 'max_features': 'auto', 'max_depth': 40, 'n_estimators': 200, 'min_samples_split': 5}\n",
      "-335596.151715 (+/-446269.424462) for {'min_samples_leaf': 2, 'max_features': 'auto', 'max_depth': 20, 'n_estimators': 1600, 'min_samples_split': 2}\n",
      "-335912.733266 (+/-436155.719666) for {'min_samples_leaf': 2, 'max_features': 'auto', 'max_depth': 80, 'n_estimators': 200, 'min_samples_split': 2}\n",
      "-336014.206119 (+/-458552.698273) for {'min_samples_leaf': 2, 'max_features': 'auto', 'max_depth': 20, 'n_estimators': 800, 'min_samples_split': 5}\n",
      "-336041.730863 (+/-439453.062669) for {'min_samples_leaf': 2, 'max_features': 'auto', 'max_depth': 100, 'n_estimators': 600, 'min_samples_split': 2}\n",
      "-336069.018226 (+/-459861.186636) for {'min_samples_leaf': 2, 'max_features': 'auto', 'max_depth': None, 'n_estimators': 600, 'min_samples_split': 5}\n",
      "-336535.438161 (+/-452874.788380) for {'min_samples_leaf': 2, 'max_features': 'auto', 'max_depth': 40, 'n_estimators': 200, 'min_samples_split': 2}\n",
      "-336782.169989 (+/-438303.269056) for {'min_samples_leaf': 2, 'max_features': 'auto', 'max_depth': None, 'n_estimators': 800, 'min_samples_split': 2}\n",
      "-336811.510547 (+/-442737.448031) for {'min_samples_leaf': 2, 'max_features': 'auto', 'max_depth': 60, 'n_estimators': 1200, 'min_samples_split': 2}\n",
      "-337050.617445 (+/-432495.183110) for {'min_samples_leaf': 2, 'max_features': 'auto', 'max_depth': None, 'n_estimators': 1200, 'min_samples_split': 2}\n",
      "-337190.804908 (+/-444768.411595) for {'min_samples_leaf': 2, 'max_features': 'auto', 'max_depth': 40, 'n_estimators': 1000, 'min_samples_split': 2}\n",
      "-337232.085267 (+/-442802.425130) for {'min_samples_leaf': 2, 'max_features': 'auto', 'max_depth': 40, 'n_estimators': 1200, 'min_samples_split': 2}\n",
      "\n",
      "Optimal value of C: \n",
      "\n",
      "{'min_samples_leaf': 2, 'max_features': 'auto', 'max_depth': 20, 'n_estimators': 600, 'min_samples_split': 5}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "show_results(rf_cv_aft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "performing GB grid search...\n",
      "Fitting 5 folds for each of 1440 candidates, totalling 7200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done 173 tasks      | elapsed:    7.6s\n",
      "[Parallel(n_jobs=4)]: Done 623 tasks      | elapsed:   30.8s\n",
      "[Parallel(n_jobs=4)]: Done 1373 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=4)]: Done 2423 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=4)]: Done 3773 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=4)]: Done 5423 tasks      | elapsed:  4.6min\n",
      "[Parallel(n_jobs=4)]: Done 7193 out of 7200 | elapsed:  6.1min remaining:    0.3s\n",
      "[Parallel(n_jobs=4)]: Done 7200 out of 7200 | elapsed:  6.2min finished\n",
      "C:\\Users\\zharr\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=KFold(n_splits=5, random_state=None, shuffle=True),\n",
       "       error_score='raise-deprecating',\n",
       "       estimator=GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.1, loss='ls', max_depth=3, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_sampl...=None, subsample=1.0, tol=0.0001,\n",
       "             validation_fraction=0.1, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid='warn', n_jobs=4,\n",
       "       param_grid={'min_samples_leaf': [1, 2, 4], 'max_features': ['auto', 'sqrt'], 'max_depth': [10, 20, 40, 60, 80, 100, 200, None], 'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000], 'min_samples_split': [2, 5, 10]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='neg_mean_squared_error', verbose=1)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GradientBoostingRegressor regressor grid search\n",
    "print('performing GB grid search...')\n",
    "\n",
    "gb_cv_aft = GridSearchCV(GradientBoostingRegressor(), param_grid, cv=KFold(5, shuffle=True), scoring='neg_mean_squared_error',\n",
    "                         verbose=1, n_jobs=4)\n",
    "gb_cv_aft.fit(X_after, y_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 15 grid search scores on the basis of mean validation accuracy: \n",
      "\n",
      "-358803.781378 (+/-323368.631638) for {'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 60, 'n_estimators': 200, 'min_samples_split': 2}\n",
      "-363906.688643 (+/-305824.223997) for {'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 80, 'n_estimators': 600, 'min_samples_split': 10}\n",
      "-366639.660541 (+/-296311.724936) for {'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 20, 'n_estimators': 1200, 'min_samples_split': 5}\n",
      "-368976.174527 (+/-280540.723398) for {'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': None, 'n_estimators': 1600, 'min_samples_split': 2}\n",
      "-369143.272186 (+/-265105.238344) for {'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': None, 'n_estimators': 1200, 'min_samples_split': 10}\n",
      "-369691.782298 (+/-328128.233905) for {'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 20, 'n_estimators': 200, 'min_samples_split': 10}\n",
      "-369739.566646 (+/-303663.342094) for {'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 10, 'n_estimators': 800, 'min_samples_split': 5}\n",
      "-370169.556417 (+/-296355.219969) for {'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 20, 'n_estimators': 1000, 'min_samples_split': 10}\n",
      "-370703.368130 (+/-238433.741079) for {'min_samples_leaf': 4, 'max_features': 'sqrt', 'max_depth': 10, 'n_estimators': 800, 'min_samples_split': 2}\n",
      "-370753.513322 (+/-257240.199953) for {'min_samples_leaf': 4, 'max_features': 'sqrt', 'max_depth': 100, 'n_estimators': 1600, 'min_samples_split': 5}\n",
      "-371686.823848 (+/-217852.290236) for {'min_samples_leaf': 4, 'max_features': 'sqrt', 'max_depth': 60, 'n_estimators': 200, 'min_samples_split': 5}\n",
      "-372384.687199 (+/-324861.533952) for {'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 80, 'n_estimators': 1200, 'min_samples_split': 5}\n",
      "-372501.804111 (+/-306308.973478) for {'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 80, 'n_estimators': 1800, 'min_samples_split': 2}\n",
      "-372960.724107 (+/-246484.934723) for {'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 10, 'n_estimators': 1200, 'min_samples_split': 10}\n",
      "-373939.601099 (+/-317192.706678) for {'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 60, 'n_estimators': 1000, 'min_samples_split': 10}\n",
      "\n",
      "Optimal value of C: \n",
      "\n",
      "{'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 60, 'n_estimators': 200, 'min_samples_split': 2}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "show_results(gb_cv_aft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
